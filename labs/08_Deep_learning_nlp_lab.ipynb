{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Python and Natural Language Technologies\n",
    "\n",
    "__Laboratory 08, Deep learning and NLP__\n",
    "\n",
    "__November 05, 2020__\n",
    "\n",
    "__Ádám Kovács__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "Pytorch has a special framework to work with textual data, called [TorchText](https://pytorch.org/text/), it has a lot of useful built in methods and datasets.\n",
    "\n",
    "The Field class is one of the main concepts of TorchText.\n",
    "\n",
    "The parameters of a Field specify how the data should be processed.\n",
    "\n",
    "The TEXT field describes how the articles need to be processed, also the LABEL field is to process the label of the articles.\n",
    "\n",
    "We can pass tokenize='spacy' to our tokenizer to use spacy's methods. The default tokenizer would be splitting on spaces.\n",
    "\n",
    "LABEL is defined by a LabelField, a special subset of the Field class specifically used for handling labels.\n",
    "\n",
    "For more on [Fields](https://torchtext.readthedocs.io/en/latest/data.html)\n",
    "\n",
    "Random seeds are used for reproducibility.\n",
    "\n",
    "__NOTE: it is advised to use Google Colab for this laboratory. If you have completed the exercises, you can download the notebook and upload it to the repository__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchtext==0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext.datasets import text_classification\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and extract the dataset we produced during the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = os.getenv(\"data\")\n",
    "if data_dir is None:\n",
    "    data_dir = \"\"\n",
    "\n",
    "ml_path = os.path.join(data_dir, \"data.zip\")\n",
    "\n",
    "if not os.path.exists(ml_path):\n",
    "    print(\"Download data\")\n",
    "    import urllib\n",
    "    u = urllib.request.URLopener()\n",
    "    u.retrieve(\"http://sandbox.hlt.bme.hu/~adaamko/dataset.zip\", ml_path)\n",
    "\n",
    "unzip_path = os.path.join(data_dir, \"data\")\n",
    "\n",
    "if not os.path.exists(unzip_path):\n",
    "    print(\"Unzip data\")\n",
    "    from zipfile import ZipFile\n",
    "    with ZipFile(ml_path) as myzip:\n",
    "        myzip.extractall(data_dir)\n",
    "\n",
    "data_dir = unzip_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize='spacy')\n",
    "LABEL = data.LabelField(dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Use data.TabularDataset to load in the dataset and split the train file into train and dev. Use random.seed(SEED) when splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d04c8552ae76e66d60da7f14a92d6d03",
     "grade": true,
     "grade_id": "cell-dd8ad946e85af83d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(TEXT, LABEL):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = load_dataset(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training examples: {len(train)}')\n",
    "print(f'Number of validation examples: {len(valid)}')\n",
    "print(f'Number of testing examples: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train) == 84000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building vocabulary is an essential step to handle textual data. This is a lookup table where every unique word has a corresponding index.\n",
    "\n",
    "This is done so our machine learning model can operate on numbers instead of strings. The indexes are then used to construct embeddings for our words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train)  \n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a special unknown or __< unk >__ token. For example, if the sentence was \"This film is great and I love it\" but if the word \"love\" is not in the vocab, it would become \"This film is great and I __< unk >__ it\".\n",
    "    \n",
    "We feed batches into our model. And we feed one batch at a time. Within a batch all sentences need to be in the same size. We need to ensure that each sentence in the batch is the same size, and the shorter ones are padded.\n",
    "\n",
    "![pad](https://github.com/bentrevett/pytorch-sentiment-analysis/raw/79bb86abc9e89951a5f8c4a25ca5de6a491a4f5d/assets/sentiment6.png)\n",
    "\n",
    "_(image from bentrevett)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Use data.BucketIterator to consturct iterators on training, dev and split data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e865f68f5b7394451fca7b53834bdeb0",
     "grade": true,
     "grade_id": "cell-8693c7903488bb75",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def construct_iterators(train, dev, test):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = construct_iterators(\n",
    "    train, valid, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build a LSTM model that predicts the label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints:\n",
    "Each batch, text, is a tensor of size [sentence length, batch size].\n",
    "\n",
    "The input batch is then passed through the embedding layer to get word embeddings. Then, the embedded layer is then fed into the LSTM..\n",
    "\n",
    "The LSTM returns 2 tensors, output of size [sentence length, batch size, hidden dim] and hidden of size [1, batch size, hidden dim]. We take the last layer of the output.\n",
    "\n",
    "Finally, we feed the output of lstm through the linear layer, fc, to produce a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "963b057b5a0080131f611434c7320431",
     "grade": true,
     "grade_id": "cell-ec58f455b73c3e8a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import autograd\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, text):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 100\n",
    "OUTPUT_DIM = 4\n",
    "\n",
    "model = LSTMClassifier(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the learning rate optimizer, you can experiment with various optimizers: https://pytorch.org/docs/stable/optim.html\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Send the tensors to GPU if available\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def class_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch\n",
    "    \"\"\"\n",
    "    rounded_preds = preds.argmax(1)\n",
    "    correct = (rounded_preds == y).float()  # convert into float for division\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Implement the train and the evaluate functions.\n",
    "\n",
    "train should :\n",
    "- iterate throught the dataset with the given iterator, \n",
    "- get the output from the model\n",
    "- calculate the loss and the accuracy\n",
    "- Propagate backward the loss\n",
    "- And calculate epoch loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfb6fc580a0311c4829cec7c6ef231ad",
     "grade": true,
     "grade_id": "cell-37ee28b2da6ff433",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate should:\n",
    "- set the model to .eval() mode\n",
    "- with torch.no_grad(), iterate on the iterator\n",
    "- calculate the prediction and the loss on the validation dataset\n",
    "- calculate epoch loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9fdc0958d7915c940f16dc403e70f891",
     "grade": true,
     "grade_id": "cell-fe0cb870cbba4926",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 15\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Add pretrained embeddings to the model instead of training it.\n",
    "\n",
    "- You need to set vectors to the vocabulary (https://pytorch.org/text/vocab.html)\n",
    "- Set the embeddings weights in __init__\n",
    "- Turn off the training of the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85e9ba1345bf796bba599181ecded510",
     "grade": true,
     "grade_id": "cell-dee7a285c9328339",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------------------------------------PASSING LEVEL---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. After a few epochs, the model starts to overfit (the accuracy goes down in dev and up in train). Add Dropout layers to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Change LSTM to Bi-LSTM\n",
    "- See: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "- Be aware that Bi-LSTM produces two outputs, so the shape will also be 2*hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------------------------------------EXTRA LEVEL-----------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
